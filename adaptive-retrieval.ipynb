{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using adaptive retrieval on the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "\n",
    "seed = 633\n",
    "np.random.seed(seed)\n",
    "import random\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "# take the results of parametric and nonparametric-augmented systems \n",
    "# and compute how well adaptive retrieval would perform\n",
    "do_plot = False\n",
    "n_boot = 100\n",
    "parametric_path = r\"D:\\work\\Research\\d0\\reference\\adaptive-retrieval\\results\\temp\\model=EleutherAI_gpt-neox-20b-input=None-method=vanilla-shots=15-n=1000_int8bit.csv\"  # ADD PATH TO VANILLA RESULTS\n",
    "nonparametric_path = r\"D:\\work\\Research\\d0\\reference\\adaptive-retrieval\\results\\temp\\model=EleutherAI_gpt-neox-20b-input=None-method=BM25-shots=15-n=1000_int8bit.csv\"  # ADD PATH TO RETRIEVAL-AUGMENTED RESULTS\n",
    "def clean(df):\n",
    "    return df[~df[\"s_pop\"].isna() & (df[\"s_pop\"] >= 0)]\n",
    "sample = clean(pd.read_csv(parametric_path))\n",
    "sample_ret = clean(pd.read_csv(nonparametric_path))\n",
    "sample = sample.sort_values(\"question\").reset_index(drop=True)\n",
    "sample_ret = sample_ret.sort_values(\"question\").reset_index(drop=True)\n",
    "print(len(sample), len(sample_ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_ret_accs_all=0.3\n",
      "test_ret_accs_all=0.308\n",
      "test_ret_accs_all=0.272\n",
      "test_ret_accs_all=0.328\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.296\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.356\n",
      "test_ret_accs_all=0.288\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.268\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.312\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.272\n",
      "test_ret_accs_all=0.292\n",
      "test_ret_accs_all=0.272\n",
      "test_ret_accs_all=0.288\n",
      "test_ret_accs_all=0.244\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.24\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.3\n",
      "test_ret_accs_all=0.256\n",
      "test_ret_accs_all=0.288\n",
      "test_ret_accs_all=0.296\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.26\n",
      "test_ret_accs_all=0.28\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.276\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.284\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.32\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.284\n",
      "test_ret_accs_all=0.316\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.28\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.26\n",
      "test_ret_accs_all=0.312\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.3\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.228\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.28\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.3\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.304\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.268\n",
      "test_ret_accs_all=0.296\n",
      "test_ret_accs_all=0.248\n",
      "test_ret_accs_all=0.272\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.26\n",
      "test_ret_accs_all=0.32\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.288\n",
      "test_ret_accs_all=0.308\n",
      "test_ret_accs_all=0.264\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "test_ret_accs_all=0.292\n",
      "test_ret_accs\n",
      "test_ret_accs_all=nan\n",
      "prop adaptive: 1.0\n",
      "prop retrieval: 0.68492\n",
      "parametric knowledge only: 0.18845454545454546\n",
      "retrieval augmented accuracy: 0.2858181818181818\n",
      "hybrid accuracy: 0.2795454545454546 pm 0.00295462097805572\n",
      "hybrid accuracy on train: 0.3043262411347518\n",
      "[np.float64(0.30133333333333334), np.float64(0.30133333333333334), np.float64(0.31333333333333335), np.float64(0.28933333333333333), np.float64(0.31066666666666665), np.float64(0.31066666666666665), np.float64(0.30133333333333334), np.float64(0.30266666666666664), np.float64(0.2826666666666667), np.float64(0.304), np.float64(0.308), np.float64(0.30266666666666664), np.float64(0.316), np.float64(0.3), np.float64(0.312), np.float64(0.30933333333333335), np.float64(0.3), np.float64(0.312), np.float64(0.29733333333333334), np.float64(0.30266666666666664), np.float64(0.31066666666666665), np.float64(0.31466666666666665), np.float64(0.312), np.float64(0.2986666666666667), np.float64(0.308), np.float64(0.30933333333333335), np.float64(0.3), np.float64(0.3), np.float64(0.308), np.float64(0.31333333333333335), np.float64(0.32133333333333336), np.float64(0.292), np.float64(0.292), np.float64(0.296), np.float64(0.30533333333333335), np.float64(0.312), np.float64(0.308), np.float64(0.30133333333333334), np.float64(0.31333333333333335), np.float64(0.3), np.float64(0.308), np.float64(0.308), np.float64(0.3), np.float64(0.304), np.float64(0.3), np.float64(0.316), np.float64(0.31066666666666665), np.float64(0.292), np.float64(0.304), np.float64(0.29733333333333334), np.float64(0.30533333333333335), np.float64(0.29733333333333334), np.float64(0.31333333333333335), np.float64(0.284), np.float64(0.2946666666666667), np.float64(0.2946666666666667), np.float64(0.316), np.float64(0.2946666666666667), np.float64(0.30533333333333335), np.float64(0.29733333333333334), np.float64(0.312), np.float64(0.30266666666666664), np.float64(0.31333333333333335), np.float64(0.324), np.float64(0.31333333333333335), np.float64(0.29333333333333333), np.float64(0.29333333333333333), np.float64(0.30933333333333335), np.float64(0.30533333333333335), np.float64(0.2946666666666667), np.float64(0.30933333333333335), np.float64(0.31466666666666665), np.float64(0.30266666666666664), np.float64(0.32133333333333336), np.float64(0.2986666666666667), np.float64(0.31066666666666665), np.float64(0.3), np.float64(0.31066666666666665), np.float64(0.30266666666666664), np.float64(0.31066666666666665), np.float64(0.29733333333333334), np.float64(0.31733333333333336), np.float64(0.31333333333333335), np.float64(0.296), np.float64(0.292), np.float64(0.30533333333333335), np.float64(0.292), np.float64(0.312), np.float64(0.3), np.float64(0.296), np.float64(0.308), np.float64(0.2986666666666667), np.float64(0.30933333333333335), np.float64(0.2906666666666667)]\n",
      "overall accuracy gain: -0.006272727272727208\n",
      "overall accuracy: 0.2795454545454546\n"
     ]
    }
   ],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "props = sample.prop.unique()\n",
    "\n",
    "test_ret_accs_all = []\n",
    "test_param_accs_all = []\n",
    "test_hybrid_accs_all = []\n",
    "test_count_rets = []\n",
    "test_count_params = []\n",
    "train_hybrid_accs_all = []\n",
    "\n",
    "for boot in range(1 if do_plot else n_boot): # boot = 1~100\n",
    "    split_proportion = 0.75\n",
    "    # 結果の件数＊0.75個の\"train\" と 結果の件数*0.25の\"test\"のリスト\n",
    "    split_mask = [\"train\"] * round(len(sample) * split_proportion) + [\"test\"] * round(len(sample) * (1 - split_proportion))\n",
    "    np.random.shuffle(split_mask)\n",
    "    # カラムを追加 各データに trainかtestを割り当てる\n",
    "    sample[\"split\"] = split_mask\n",
    "    sample_ret[\"split\"] = split_mask\n",
    "\n",
    "    test_ret_accs = []\n",
    "    test_param_accs = []\n",
    "    test_hybrid_accs = []\n",
    "    train_hybrid_accs = []\n",
    "    test_count_rets.append(0)\n",
    "    test_count_params.append(0)\n",
    "    test_sizes = []\n",
    "    train_sizes = []\n",
    "    train_threshs = dict()\n",
    "    plot_title = \"\"\n",
    "    if do_plot:\n",
    "        plt.figure(dpi=200, figsize=(30, 30))\n",
    "    # 各propについて見ていく propとはrelationship typeのこと。\n",
    "    for i, prop in enumerate(props):\n",
    "\n",
    "        if do_plot:\n",
    "            plt.subplot(4, 4, i+1)\n",
    "        cluster_sample = sample[sample.prop == prop].copy()\n",
    "        cluster_sample_ret_with_pop = sample_ret[sample_ret.prop == prop].copy()\n",
    "\n",
    "        # s_popの対数を出す。検索なし・ありで同じ値なのでなしの場合だけ計算すればＯＫ\n",
    "        log_pop = np.log(cluster_sample[\"s_pop\"].values)\n",
    "        cluster_sample[\"log_pop\"] = log_pop\n",
    "        cluster_sample_ret_with_pop[\"log_pop\"] = log_pop\n",
    "        ser = log_pop\n",
    "        _, bin_edges = np.histogram(ser) # 人気度の範囲\n",
    "\n",
    "        # 正解・不正解の件数を人気度をbinsにカウントする\n",
    "        c = cluster_sample.is_correct.values\n",
    "        counts_c, _ = np.histogram(ser[c], bins=bin_edges)\n",
    "        counts_inc, _ = np.histogram(ser[~c], bins=bin_edges)\n",
    "        total = counts_c + counts_inc\n",
    "\n",
    "        c_ret = cluster_sample_ret_with_pop.is_correct.values\n",
    "        counts_c_ret, _ = np.histogram(ser[c_ret], bins=bin_edges)\n",
    "        counts_inc_ret, _ = np.histogram(ser[~c_ret], bins=bin_edges)\n",
    "        total_ret = counts_c_ret + counts_inc_ret\n",
    "\n",
    "        # binsの１区間の範囲*0.4\n",
    "        width = 0.4*(bin_edges[1] - bin_edges[0])\n",
    "        # 人気度の閾値を計算（最も正解数が多くなるiを採用）\n",
    "        thresh_idx = np.argmax(list((sum(counts_c_ret[:i]) + sum(counts_c[i:])) / sum(total_ret) for i in range(len(total_ret) + 1)))\n",
    "\n",
    "        # plt.bar(bin_edges[:-1] - 0.5 * width, counts_c / total, width=width, alpha=0.9, label=\"parametric knowledge (vanilla)\", align='edge')\n",
    "        # plt.bar(bin_edges[:-1] + 0.5 * width, counts_c_ret / total_ret, width=width, alpha=0.9, label=\"retrieval augmented (BM25)\", align='edge')\n",
    "        # lo, hi = proportion_confint(counts_c, total, alpha=0.05, method='wilson')\n",
    "        # plt.errorbar(bin_edges[:-1], counts_c / total, yerr=[counts_c / total - lo, hi - counts_c / total], fmt='none', ecolor='black', elinewidth=1, capsize=2)\n",
    "        # lo, hi = proportion_confint(counts_c_ret, total_ret, alpha=0.05, method='wilson')\n",
    "        # plt.errorbar(bin_edges[:-1] + width, counts_c_ret / total_ret, yerr=[counts_c_ret / total_ret - lo, hi - counts_c_ret / total_ret], fmt='none', ecolor='black', elinewidth=1, capsize=2)\n",
    "        # # plt.errorbar(bin_edges[:-1], counts_c / total, , fmt='none', ecolor='black', capsize=2)\n",
    "        # # plt.errorbar(bin_edges[:-1], counts_c_ret / total_ret, yerr=wilson(counts_c_ret / total_ret, total_ret), fmt='none', ecolor='black', capsize=2)\n",
    "        # plt.axvline(x=bin_edges[thresh_idx] + 0.7 * (bin_edges[1] - bin_edges[0]), color='red', linestyle='--', label=\"threshold\")\n",
    "\n",
    "        # 正解率\n",
    "        param_acc = sum(counts_c) / sum(total)\n",
    "        ret_acc = sum(counts_c_ret) / sum(total_ret)\n",
    "        hybrid_acc = (sum(counts_c_ret[:thresh_idx]) + sum(counts_c[thresh_idx:])) / (sum(total_ret[:thresh_idx]) + sum(total[thresh_idx:]))\n",
    "        ret_acc_gain = hybrid_acc - ret_acc\n",
    "        param_acc_gain = hybrid_acc - param_acc\n",
    "        \n",
    "        # let the optimal threshold be the one that maximizes the hybrid accuracy\n",
    "        train_idxs = cluster_sample.split.values == \"train\"\n",
    "        test_idxs = cluster_sample.split.values == \"test\"\n",
    "        train_ser = ser[train_idxs]\n",
    "        test_ser = ser[test_idxs]\n",
    "        train_c = c[train_idxs]\n",
    "        test_c = c[test_idxs]\n",
    "        train_c_ret = c_ret[train_idxs]\n",
    "        test_c_ret = c_ret[test_idxs]\n",
    "        train_counts_c, _ = np.histogram(train_ser[train_c], bins=bin_edges)\n",
    "        train_counts_inc, _ = np.histogram(train_ser[~train_c], bins=bin_edges)\n",
    "        train_total = train_counts_c + train_counts_inc\n",
    "        train_counts_c_ret, _ = np.histogram(train_ser[train_c_ret], bins=bin_edges)\n",
    "        train_counts_inc_ret, _ = np.histogram(train_ser[~train_c_ret], bins=bin_edges)\n",
    "        train_total_ret = train_counts_c_ret + train_counts_inc_ret\n",
    "        test_counts_c, _ = np.histogram(test_ser[test_c], bins=bin_edges)\n",
    "        test_counts_inc, _ = np.histogram(test_ser[~test_c], bins=bin_edges)\n",
    "        test_total = test_counts_c + test_counts_inc\n",
    "        test_counts_c_ret, _ = np.histogram(test_ser[test_c_ret], bins=bin_edges)\n",
    "        test_counts_inc_ret, _ = np.histogram(test_ser[~test_c_ret], bins=bin_edges)\n",
    "        test_total_ret = test_counts_c_ret + test_counts_inc_ret # こいつが0になる\n",
    "        \n",
    "        # find the optimal threshold\n",
    "        train_thresh_idx = np.argmax(list((sum(train_counts_c_ret[:i]) + sum(train_counts_c[i:])) / (sum(train_total_ret[:i]) + sum(train_total[i:])) for i in range(len(train_total) + 1)))\n",
    "        train_thresh = bin_edges[train_thresh_idx] - 0.5 * (bin_edges[1] - bin_edges[0])\n",
    "        train_threshs[prop] = train_thresh\n",
    "\n",
    "        # calculate the accuracy on the test set\n",
    "        test_param_acc = sum(test_counts_c) / sum(test_total)\n",
    "        test_ret_acc = sum(test_counts_c_ret) / sum(test_total_ret)\n",
    "        test_hybrid_acc = (sum(test_counts_c_ret[:train_thresh_idx]) + sum(test_counts_c[train_thresh_idx:])) / (sum(test_total_ret[:train_thresh_idx]) + sum(test_total[train_thresh_idx:]))\n",
    "        test_sizes.append(sum(test_total))\n",
    "        test_count_rets[-1] += sum(test_total_ret[:train_thresh_idx])\n",
    "        test_count_params[-1] += sum(test_total[train_thresh_idx:])\n",
    "        test_ret_accs.append(test_ret_acc)\n",
    "        if np.isnan(test_sizes[-1]):\n",
    "            print(\"test_sizes\")\n",
    "        if np.isnan(test_ret_accs[-1]):\n",
    "            print(f\"test_ret_accs\")\n",
    "        test_param_accs.append(test_param_acc)\n",
    "        test_hybrid_accs.append(test_hybrid_acc)\n",
    "        train_hybrid_accs.append((sum(train_counts_c_ret[:train_thresh_idx]) + sum(train_counts_c[train_thresh_idx:])) / (sum(train_total_ret[:train_thresh_idx]) + sum(train_total[train_thresh_idx:])))\n",
    "        train_sizes.append(sum(train_total))\n",
    "\n",
    "        if do_plot:\n",
    "            plt.bar(bin_edges[:-1] - 0.5 * width, test_counts_c / test_total, width=width, alpha=0.9, label=\"parametric knowledge (vanilla)\", align='edge', hatch='//')\n",
    "            plt.bar(bin_edges[:-1] + 0.5 * width, test_counts_c_ret / test_total_ret, width=width, alpha=0.9, label=\"retrieval augmented (BM25)\", align='edge', hatch='//')\n",
    "            lo, hi = proportion_confint(test_counts_c, test_total, alpha=0.05, method='wilson')\n",
    "            plt.errorbar(bin_edges[:-1], test_counts_c / test_total, yerr=[test_counts_c / test_total - lo, hi - test_counts_c / test_total], fmt='none', ecolor='black', elinewidth=1, capsize=2)\n",
    "            lo, hi = proportion_confint(test_counts_c_ret, test_total_ret, alpha=0.05, method='wilson')\n",
    "            plt.errorbar(bin_edges[:-1] + width, test_counts_c_ret / test_total_ret, yerr=[test_counts_c_ret / test_total_ret - lo, hi - test_counts_c_ret / test_total_ret], fmt='none', ecolor='black', elinewidth=1, capsize=2)\n",
    "            plt.axvline(x=bin_edges[train_thresh_idx] - 0.8 * width, color='red', linestyle='--', label=\"threshold\")\n",
    "\n",
    "            print(f\"Threshold for {prop}:\", train_thresh)\n",
    "            print(\"Parametric knowledge only:\", param_acc)\n",
    "            print(\"Retrieval augmented accuracy:\", ret_acc)\n",
    "            print(f\"New accuracy with thresh={thresh_idx}:\", hybrid_acc)\n",
    "            print()\n",
    "            plt.title(f\"{prop}\")\n",
    "            plt.ylim([0,1.01])\n",
    "    if do_plot:\n",
    "        plt.xlabel(\"log(s_pop)\")\n",
    "        plt.ylabel(\"proportion correct\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # take the weighted mean by test_size\n",
    "    test_ret_accs_all.append(np.average(test_ret_accs, weights=test_sizes))\n",
    "    if test_ret_accs_all[-1] is np.nan:\n",
    "        print(test_param_accs)\n",
    "        print(test_sizes)\n",
    "    print(\"test_ret_accs_all={}\".format(test_ret_accs_all[-1]))\n",
    "    test_param_accs_all.append(np.average(test_param_accs, weights=test_sizes))\n",
    "    test_hybrid_accs_all.append(np.average(test_hybrid_accs, weights=test_sizes))\n",
    "    train_hybrid_accs_all.append(np.average(train_hybrid_accs, weights=train_sizes))\n",
    "\n",
    "# nan対策の妥協案 TODO:削除予定\n",
    "import math\n",
    "test_count_rets = [x for x in test_count_rets if math.isnan(x) == False]\n",
    "test_count_params = [x for x in test_count_params if math.isnan(x) == False]\n",
    "test_param_accs_all = [x for x in test_param_accs_all if math.isnan(x) == False]\n",
    "test_ret_accs_all = [x for x in test_ret_accs_all if math.isnan(x) == False]\n",
    "test_hybrid_accs_all = [x for x in test_hybrid_accs_all if math.isnan(x) == False]\n",
    "train_hybrid_accs_all = [x for x in train_hybrid_accs_all if math.isnan(x) == False]\n",
    "\n",
    "test_size = split_mask.count(\"test\")\n",
    "prop_adaptive = (np.mean(test_count_rets) + np.mean(test_count_params)) / test_size\n",
    "param_acc = np.mean(test_param_accs_all)\n",
    "# print(test_param_accs_all)\n",
    "# print(len(test_param_accs_all))\n",
    "ret_acc = np.mean(test_ret_accs_all)\n",
    "hybrid_acc = np.mean(test_hybrid_accs_all)\n",
    "train_hybrid_acc = np.mean(train_hybrid_accs_all)\n",
    "overall_hybrid_acc = np.mean(test_hybrid_accs_all) * prop_adaptive + max(np.mean(test_param_accs_all), np.mean(test_ret_accs_all)) * (1 - prop_adaptive)\n",
    "sem_test_hybrid_acc = 2 * np.std(test_hybrid_accs_all) / np.sqrt(test_size)\n",
    "print(\"prop adaptive:\", prop_adaptive)\n",
    "print(\"prop retrieval:\", np.mean(test_count_rets) / test_size)\n",
    "print(\"parametric knowledge only:\", param_acc)\n",
    "print(\"retrieval augmented accuracy:\", ret_acc)\n",
    "print(\"hybrid accuracy:\", hybrid_acc, \"pm\", sem_test_hybrid_acc)\n",
    "print(\"hybrid accuracy on train:\", train_hybrid_acc)\n",
    "print(\"overall accuracy gain:\", overall_hybrid_acc - max(param_acc, ret_acc))\n",
    "print(\"overall accuracy:\", overall_hybrid_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# memo\n",
    "prop adaptive: 1.0\n",
    "\n",
    "prop retrieval: 0.7981889543033361\n",
    "\n",
    "parametric knowledge only: 0.17163442668909443\n",
    "\n",
    "retrieval augmented accuracy: 0.2537706756377908\n",
    "\n",
    "hybrid accuracy: 0.2706980656013457 pm 0.000232407510793975\n",
    "\n",
    "hybrid accuracy on train: 0.2719317757009346\n",
    "\n",
    "overall accuracy gain: 0.016927389963554862\n",
    "\n",
    "overall accuracy: 0.2706980656013457"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
